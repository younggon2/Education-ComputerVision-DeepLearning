{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day2-2 CNN (Segmentation).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEeUj8VKuUeQ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYdtrCh0uYc3"
      },
      "source": [
        "# Semantic Segmentation\n",
        "![대체 텍스트](https://miro.medium.com/max/700/1*8Nwk_IdGpe235Nsfewpucg.png)\n",
        "\n",
        "from Fei-Fei Li Stanford Course — Detection And Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia2ncRYx9vcw"
      },
      "source": [
        "## STEP 1: 데이터셋과 읽어들이기\n",
        "https://www.kaggle.com/nikhilpandey360/lung-segmentation-from-chest-x-ray-dataset\n",
        "\n",
        "![대체 텍스트](https://www.altoros.com/blog/wp-content/uploads/2018/12/segmentation-results-max-dice-score.png)\n",
        "\n",
        "**Lung_Segmentation.zip**  \n",
        "**256x256x3**  \n",
        "**566 [image, label]**  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuYyXTgi2sIM"
      },
      "source": [
        "!wget https://github.com/younggon2/Education-ComputerVision-DeepLearning/raw/master/res/Lung_Segmentation.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87yxHFkn7ntm"
      },
      "source": [
        "!unzip Lung_Segmentation.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MdVbVou8nYe"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "IMG_CHANNELS = 3\n",
        "\n",
        "data_path = \"./\"\n",
        "\n",
        "files = os.listdir(os.path.join(data_path, 'image'))\n",
        "file_headers = []  #python list\n",
        "for f in files:\n",
        "    f1 = os.path.splitext(f)[0]\n",
        "    file_headers.append(f1)\n",
        "#print(file_headers)\n",
        "\n",
        "X_all = np.zeros((len(file_headers), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "y_all = np.zeros((len(file_headers), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
        "\n",
        "count = 0\n",
        "for fh in file_headers:\n",
        "    f1 = os.path.join(data_path, 'image', '{}.png'.format(fh))\n",
        "    l1 = os.path.join(data_path, 'label', '{}.png'.format(fh))\n",
        "\n",
        "    img = imread(f1)[:,:,:IMG_CHANNELS]\n",
        "    mask = imread(l1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "\n",
        "    X_all[count] = img\n",
        "    y_all[count] = mask\n",
        "\n",
        "    count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BznnT3Adwbfu"
      },
      "source": [
        "**딥러닝을 위한 데이터 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSTQEB5RvOv5"
      },
      "source": [
        "X_all = X_all.astype('float32') / 255.\n",
        "y_all = y_all.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8N9HrvHwfMX"
      },
      "source": [
        "**학습, 검증, 테스트 데이터 셋으로 분리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS7knnuMwesT"
      },
      "source": [
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yaOK2b8wita"
      },
      "source": [
        "print('X_train',X_train.shape)\n",
        "print('X_valid',X_valid.shape)\n",
        "print('X_test',X_test.shape)\n",
        "print('y_train',y_train.shape)\n",
        "print('y_valid',y_valid.shape)\n",
        "print('y_test',y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3je2O43iwkOQ"
      },
      "source": [
        "## STEP 2: 데이터 살펴보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgsh0Y_lwjY6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotTrainData(a,b,c):\n",
        "    for i in range(3):\n",
        "        ix = np.random.randint(0, len(a))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.title(\"X_\" + c)\n",
        "        plt.imshow(a[ix])\n",
        "        plt.axis('off')\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.title(\"y_\" + c)\n",
        "        plt.imshow(np.squeeze(b[ix]), 'gray')\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "plotTrainData(X_train,y_train, 'train')\n",
        "plotTrainData(X_valid,y_valid, 'valid')\n",
        "plotTrainData(X_test,y_test, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTx2ONaiwonG"
      },
      "source": [
        "## STEP 3: VGG16 네트워크 다시 보기\n",
        "![대체 텍스트](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kgtHnXZwmkz"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from tensorflow.keras.layers import Add\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def vgg16():\n",
        "    inputs = Input(shape=(256, 256, 3,), name=\"VGGInput\")\n",
        "\n",
        "    conv1_1 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(bn1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    pool_1 = MaxPool2D(padding='same')(bn1_2)\n",
        "\n",
        "    #(128,128)\n",
        "    conv2_1 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(pool_1)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(bn2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    pool_2 = MaxPool2D(padding='same')(bn2_2)\n",
        "\n",
        "    #(64, 64)\n",
        "    conv3_1 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(pool_2)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_2)\n",
        "    bn3_3 = BatchNormalization()(conv3_3)\n",
        "    pool_3 = MaxPool2D(padding='same')(bn3_3)\n",
        "\n",
        "    #(32, 32)\n",
        "    conv4_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_3)\n",
        "    bn4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_1)\n",
        "    bn4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_2)\n",
        "    bn4_3 = BatchNormalization()(conv4_3)\n",
        "    pool_4 = MaxPool2D(padding='same')(bn4_3)\n",
        "\n",
        "    #(16, 16)\n",
        "    conv5_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_4)\n",
        "    bn5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_1)\n",
        "    bn5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_2)\n",
        "    bn5_3 = BatchNormalization()(conv5_3)\n",
        "    pool_5 = MaxPool2D(padding='same')(bn5_3)\n",
        "\n",
        "    #(8, 8)\n",
        "    x = Conv2D(filters=4096, kernel_size=(8,8), padding='valid', activation=relu)(pool_5)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(4096, activation=relu)(x)\n",
        "    pred = Dense(1000, activation=softmax)(x)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJw0_qpxw49a"
      },
      "source": [
        "**FCN 32 - first fully convolutional network**\n",
        "![image interpolation](https://matplotlib.org/_images/interpolation_methods.png)\n",
        "**image interpolation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpL9ty-zxDOM"
      },
      "source": [
        "## STEP 4: 첫 번째 영상분할 모델 (FCN32s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SClSaetbw3Mi"
      },
      "source": [
        "from tensorflow.keras.layers import Activation\n",
        "\n",
        "def fcn32s():\n",
        "    inputs = Input(shape=(256, 256, 3,), name=\"FCNInput\")\n",
        "\n",
        "    conv1_1 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(bn1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    pool_1 = MaxPool2D(padding='same')(bn1_2)\n",
        "\n",
        "    #(128,128)\n",
        "    conv2_1 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(pool_1)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(bn2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    pool_2 = MaxPool2D(padding='same')(bn2_2)\n",
        "\n",
        "    #(64, 64)\n",
        "    conv3_1 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(pool_2)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_2)\n",
        "    bn3_3 = BatchNormalization()(conv3_3)\n",
        "    pool_3 = MaxPool2D(padding='same')(bn3_3)\n",
        "\n",
        "    #(32, 32)\n",
        "    conv4_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_3)\n",
        "    bn4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_1)\n",
        "    bn4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_2)\n",
        "    bn4_3 = BatchNormalization()(conv4_3)\n",
        "    pool_4 = MaxPool2D(padding='same')(bn4_3)\n",
        "\n",
        "    #(16, 16)\n",
        "    conv5_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_4)\n",
        "    bn5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_1)\n",
        "    bn5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_2)\n",
        "    bn5_3 = BatchNormalization()(conv5_3)\n",
        "    pool_5 = MaxPool2D(padding='same')(bn5_3)\n",
        "\n",
        "    #x = Conv2D(filters=4096, kernel_size=(8,8), padding='valid', activation=relu)(pool_5)\n",
        "    #x = Flatten()(x)\n",
        "    #x = Dense(4096, activation=relu)(x)\n",
        "    #pred = Dense(1000, activation=softmax)(x)\n",
        "\n",
        "    conv_t1 = UpSampling2D(size = (32,32))(pool_5)\n",
        "    conv_t2 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(conv_t1)\n",
        "\n",
        "    pred = Activation('sigmoid')(conv_t2)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PXvPMABxecc"
      },
      "source": [
        "![대체 텍스트](https://jinglescode.github.io/assets/img/posts/unet-03.webp)\n",
        "![대체 텍스트](https://miro.medium.com/max/875/1*Z1hkDvyhFBogT9EkzVkX2A.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgjDwo9DxORy"
      },
      "source": [
        "def dice_coef(y_true, y_pred):\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + 1) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + 1)\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1 - dice_coef(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqbIe7xexXG7"
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# build the model\n",
        "model = fcn32s()\n",
        "sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=[dice_coef])\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train,validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1)\n",
        "model.save('fcn-32s.h5')\n",
        "\n",
        "# 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['dice_coef'], 'b', label='train dice_coef')\n",
        "acc_ax.plot(hist.history['val_dice_coef'], 'g', label='val dice_coef')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('dice_coef')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a4-tyJ06c98"
      },
      "source": [
        "## STEP 5: 결과 확인하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O86o27Py5Bw"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotPredictions(X_train_, y_train_, X_valid_, y_valid_, X_test_, y_test_, simpleFCN):\n",
        "    model = simpleFCN\n",
        "\n",
        "    ix = np.random.randint(0, len(X_train_))\n",
        "    input_ = X_train_[ix:ix+1]\n",
        "    mask_ = y_train_[ix:ix+1]\n",
        "    preds_train = model.predict(input_)\n",
        "    preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"X_train\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(input_[0])\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Y_train\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(mask_[0][:,:,0], 'gray')\n",
        "    plt.subplot(1,3,3)\n",
        "    ret = model.evaluate(input_, mask_)\n",
        "    plt.title(\"Prediction: %.4f\" % (ret[1]))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(preds_train_t[0][:,:,0], 'gray')\n",
        "    plt.show()\n",
        "\n",
        "    ix = np.random.randint(0, len(X_valid_))\n",
        "    input_ = X_valid_[ix:ix+1]\n",
        "    mask_ = y_valid_[ix:ix+1]\n",
        "    preds_valid = model.predict(input_)\n",
        "    preds_valid_t = (preds_valid > 0.5).astype(np.uint8)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"X_valid\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(input_[0])\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Y_valid\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(mask_[0][:,:,0], 'gray')\n",
        "    plt.subplot(1,3,3)\n",
        "    ret = model.evaluate(input_, mask_)\n",
        "    plt.title(\"Prediction: %.4f\" % (ret[1]))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(preds_valid_t[0][:,:,0], 'gray')\n",
        "    plt.show()\n",
        "\n",
        "    ix = np.random.randint(0, len(X_test_))\n",
        "    input_ = X_test_[ix:ix+1]\n",
        "    mask_ = y_test_[ix:ix+1]\n",
        "    preds_test = model.predict(input_)\n",
        "    preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(\"X_test\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(input_[0])\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(\"Y_test\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(mask_[0][:,:,0], 'gray')\n",
        "    plt.subplot(1,3,3)\n",
        "    ret = model.evaluate(input_, mask_)\n",
        "    plt.title(\"Prediction: %.4f\" % (ret[1]))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(preds_test_t[0][:,:,0], 'gray')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdUp6QkR6f3F"
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YstGOtio6p1F"
      },
      "source": [
        "## STEP 6: 두 번째 모델 (FCN8s)\n",
        "**Skip Connection**\n",
        "![대체 텍스트](http://deeplearning.net/tutorial/_images/fcn_schema.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juzQhEGf6iA0"
      },
      "source": [
        "def fcn8s():\n",
        "    inputs = Input(shape=(256, 256, 3,), name=\"FCNInput\")\n",
        "\n",
        "    conv1_1 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(bn1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    pool_1 = MaxPool2D(padding='same')(bn1_2)\n",
        "\n",
        "    #(128,128)\n",
        "    conv2_1 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(pool_1)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(bn2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    pool_2 = MaxPool2D(padding='same')(bn2_2)\n",
        "\n",
        "    #(64, 64)\n",
        "    conv3_1 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(pool_2)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_2)\n",
        "    bn3_3 = BatchNormalization()(conv3_3)\n",
        "    pool_3 = MaxPool2D(padding='same')(bn3_3)\n",
        "\n",
        "    #(32, 32)\n",
        "    conv4_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_3)\n",
        "    bn4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_1)\n",
        "    bn4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_2)\n",
        "    bn4_3 = BatchNormalization()(conv4_3)\n",
        "    pool_4 = MaxPool2D(padding='same')(bn4_3)\n",
        "\n",
        "    #(16, 16)\n",
        "    conv5_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_4)\n",
        "    bn5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_1)\n",
        "    bn5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_2)\n",
        "    bn5_3 = BatchNormalization()(conv5_3)\n",
        "    pool_5 = MaxPool2D(padding='same')(bn5_3)\n",
        "\n",
        "    #(8, 8)\n",
        "    #x = Conv2D(filters=4096, kernel_size=(8,8), padding='valid', activation=relu)(pool_5)\n",
        "    #x = Flatten()(x)\n",
        "    #x = Dense(4096, activation=relu)(x)\n",
        "    #pred = Dense(1000, activation=softmax)(x)\n",
        "\n",
        "    #(8, 8)\n",
        "    conv6 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(pool_5)\n",
        "    conv7 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(conv6)\n",
        "    conv8 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(conv7)\n",
        "\n",
        "    #(16, 16)\n",
        "    score_pool4 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_4)\n",
        "    conv_t1 = UpSampling2D(size = (2,2))(conv8)\n",
        "    fuse_1 = Add()([conv_t1,score_pool4])\n",
        "\n",
        "    #(32, 32)\n",
        "    score_pool3 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_3)\n",
        "    conv_t2 = UpSampling2D(size = (2,2))(fuse_1)\n",
        "    fuse_2 = Add()([conv_t2,score_pool3])\n",
        "\n",
        "    conv_t3 = UpSampling2D(size = (8,8))(fuse_2)\n",
        "\n",
        "    pred = Activation('sigmoid')(conv_t3)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rxdsXcp60Us"
      },
      "source": [
        "# build the model\n",
        "model = fcn8s()\n",
        "sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=[dice_coef])\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1)\n",
        "model.save('fcn-8s.h5')\n",
        "\n",
        "# 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['dice_coef'], 'b', label='train dice_coef')\n",
        "acc_ax.plot(hist.history['val_dice_coef'], 'g', label='val dice_coef')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('dice_coef')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33AyQxGc6_Nc"
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaK2SdW_8mEp"
      },
      "source": [
        "## STEP 6-1: 두 번째 모델 (FCN8s)의 개선시도 -> (FCN2s)\n",
        "**Skip Connection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdwvS_JJ8jJs"
      },
      "source": [
        "def fcn2s():\n",
        "    inputs = Input(shape=(256, 256, 3,), name=\"FCNInput\")\n",
        "\n",
        "    conv1_1 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(bn1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    pool_1 = MaxPool2D(padding='same')(bn1_2)\n",
        "\n",
        "    #(128,128)\n",
        "    conv2_1 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(pool_1)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(bn2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    pool_2 = MaxPool2D(padding='same')(bn2_2)\n",
        "\n",
        "    #(64, 64)\n",
        "    conv3_1 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(pool_2)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_2)\n",
        "    bn3_3 = BatchNormalization()(conv3_3)\n",
        "    pool_3 = MaxPool2D(padding='same')(bn3_3)\n",
        "\n",
        "    #(32, 32)\n",
        "    conv4_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_3)\n",
        "    bn4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_1)\n",
        "    bn4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_2)\n",
        "    bn4_3 = BatchNormalization()(conv4_3)\n",
        "    pool_4 = MaxPool2D(padding='same')(bn4_3)\n",
        "\n",
        "    #(16, 16)\n",
        "    conv5_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_4)\n",
        "    bn5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_1)\n",
        "    bn5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_2)\n",
        "    bn5_3 = BatchNormalization()(conv5_3)\n",
        "    pool_5 = MaxPool2D(padding='same')(bn5_3)\n",
        "\n",
        "    #(8, 8)\n",
        "    #x = Conv2D(filters=4096, kernel_size=(8,8), padding='valid', activation=relu)(pool_5)\n",
        "    #x = Flatten()(x)\n",
        "    #x = Dense(4096, activation=relu)(x)\n",
        "    #pred = Dense(1000, activation=softmax)(x)\n",
        "\n",
        "    #(8, 8)\n",
        "    conv6 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(pool_5)\n",
        "    conv7 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(conv6)\n",
        "    conv8 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(conv7)\n",
        "\n",
        "    #(16, 16)\n",
        "    score_pool4 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_4)\n",
        "    conv_t1 = UpSampling2D(size = (2,2))(conv8)\n",
        "    fuse_1 = Add()([conv_t1,score_pool4])\n",
        "\n",
        "    #(32, 32)\n",
        "    score_pool3 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_3)\n",
        "    conv_t2 = UpSampling2D(size = (2,2))(fuse_1)\n",
        "    fuse_2 = Add()([conv_t2,score_pool3])\n",
        "\n",
        "    #(64, 64)\n",
        "    score_pool2 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_2)\n",
        "    conv_t3 = UpSampling2D(size = (2,2))(fuse_2)\n",
        "    fuse_3 = Add()([conv_t3,score_pool2])\n",
        "\n",
        "    #(128, 128)\n",
        "    score_pool1 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_1)\n",
        "    conv_t4 = UpSampling2D(size = (2,2))(fuse_3)\n",
        "    fuse_4 = Add()([conv_t4,score_pool1])\n",
        "\n",
        "    conv_t5 = UpSampling2D(size = (2,2))(fuse_4)\n",
        "\n",
        "    pred = Activation('sigmoid')(conv_t5)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z03yskBN8urM"
      },
      "source": [
        "# build the model\n",
        "model = fcn2s()\n",
        "sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=[dice_coef])\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1)\n",
        "model.save('fcn-2s.h5')\n",
        "\n",
        "# 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['dice_coef'], 'b', label='train dice_coef')\n",
        "acc_ax.plot(hist.history['val_dice_coef'], 'g', label='val dice_coef')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('dice_coef')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psa5FcIz9DPt"
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvuYSFz89FLH"
      },
      "source": [
        "## STEP 7: 세 번째 모델 (FCN8s with deconvolution)\n",
        "**deconvolution**  \n",
        "![대체 텍스트](https://miro.medium.com/max/1086/1*AbCrAqPBfkqGRdhKtiZQqA.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAD5MkIu9D1s"
      },
      "source": [
        "def fcn8s_deconv():\n",
        "    inputs = Input(shape=(256, 256, 3,), name=\"FCNInput\")\n",
        "\n",
        "    conv1_1 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
        "    bn1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(bn1_1)\n",
        "    bn1_2 = BatchNormalization()(conv1_2)\n",
        "    pool_1 = MaxPool2D(padding='same')(bn1_2)\n",
        "\n",
        "    #(128,128)\n",
        "    conv2_1 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(pool_1)\n",
        "    bn2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(bn2_1)\n",
        "    bn2_2 = BatchNormalization()(conv2_2)\n",
        "    pool_2 = MaxPool2D(padding='same')(bn2_2)\n",
        "\n",
        "    #(64, 64)\n",
        "    conv3_1 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(pool_2)\n",
        "    bn3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_1)\n",
        "    bn3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(bn3_2)\n",
        "    bn3_3 = BatchNormalization()(conv3_3)\n",
        "    pool_3 = MaxPool2D(padding='same')(bn3_3)\n",
        "\n",
        "    #(32, 32)\n",
        "    conv4_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_3)\n",
        "    bn4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_1)\n",
        "    bn4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn4_2)\n",
        "    bn4_3 = BatchNormalization()(conv4_3)\n",
        "    pool_4 = MaxPool2D(padding='same')(bn4_3)\n",
        "\n",
        "    #(16, 16)\n",
        "    conv5_1 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(pool_4)\n",
        "    bn5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_1)\n",
        "    bn5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(bn5_2)\n",
        "    bn5_3 = BatchNormalization()(conv5_3)\n",
        "    pool_5 = MaxPool2D(padding='same')(bn5_3)\n",
        "\n",
        "    #(8, 8)\n",
        "    #x = Conv2D(filters=4096, kernel_size=(8,8), padding='valid', activation=relu)(pool_5)\n",
        "    #x = Flatten()(x)\n",
        "    #x = Dense(4096, activation=relu)(x)\n",
        "    #pred = Dense(1000, activation=softmax)(x)\n",
        "\n",
        "    #(8, 8)\n",
        "    conv6 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(pool_5)\n",
        "    conv7 = Conv2D(2048, (1,1), padding=\"valid\", activation=\"relu\")(conv6)\n",
        "    conv8 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(conv7)\n",
        "\n",
        "    #(16, 16)\n",
        "    score_pool4 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_4)\n",
        "\n",
        "    conv_t1 = Conv2DTranspose(1, kernel_size=(2,2), strides=(2,2), padding=\"same\")(conv8)\n",
        "    conv_t1 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t1)\n",
        "    conv_t1 = BatchNormalization()(conv_t1)\n",
        "    conv_t1 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t1)\n",
        "    conv_t1 = BatchNormalization()(conv_t1)\n",
        "\n",
        "    fuse_1 = Add()([conv_t1,score_pool4])\n",
        "\n",
        "    #(32, 32)\n",
        "    score_pool3 = Conv2D(1, (1,1), padding=\"valid\", activation=\"relu\")(pool_3)\n",
        "\n",
        "    conv_t2 = Conv2DTranspose(1, kernel_size=(2,2), strides=(2,2),padding=\"same\")(fuse_1)\n",
        "    conv_t2 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t2)\n",
        "    conv_t2 = BatchNormalization()(conv_t2)\n",
        "    conv_t2 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t2)\n",
        "    conv_t2 = BatchNormalization()(conv_t2)\n",
        "\n",
        "    fuse_2 = Add()([conv_t2,score_pool3])\n",
        "\n",
        "    #(32, 32) x 8 = 256\n",
        "    conv_t3 = Conv2DTranspose(1, kernel_size=(8,8), strides=(8,8), padding=\"same\")(fuse_2)\n",
        "    conv_t3 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t3)\n",
        "    conv_t3 = BatchNormalization()(conv_t3)\n",
        "    conv_t3 = Conv2D(1, (3, 3), activation='relu', padding='same')(conv_t3)\n",
        "    conv_t3 = BatchNormalization()(conv_t3)\n",
        "\n",
        "    pred = Activation('sigmoid')(conv_t3)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRQQjazg9Sur"
      },
      "source": [
        "# build the model\n",
        "model = fcn8s_deconv()\n",
        "sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=[dice_coef])\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1)\n",
        "model.save('fcn-8s_deconv.h5')\n",
        "\n",
        "# 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['dice_coef'], 'b', label='train dice_coef')\n",
        "acc_ax.plot(hist.history['val_dice_coef'], 'g', label='val dice_coef')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('dice_coef')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMk8V_2A9bJz"
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlOR1l839eHQ"
      },
      "source": [
        "## STEP 8: 마지막 모델 (U-Net)\n",
        "**concatenation**  \n",
        "![대체 텍스트](https://www.renom.jp/notebooks/tutorial/image_processing/u-net/unet.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwbr1FaF9dUL"
      },
      "source": [
        "from tensorflow.keras.layers import Concatenate\n",
        "\n",
        "def unet(input_size=(256,256,3)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPool2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPool2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    pool3 = MaxPool2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    pool4 = MaxPool2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "\n",
        "    up6 = Concatenate()([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4])\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "\n",
        "    up7 = Concatenate()([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3])\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "\n",
        "    up8 = Concatenate()([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2])\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
        "    conv8 = BatchNormalization()(conv8)\n",
        "\n",
        "    up9 = Concatenate()([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1])\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
        "    conv9 = BatchNormalization()(conv9)\n",
        "\n",
        "    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n",
        "\n",
        "    return Model(inputs=[inputs], outputs=[conv10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEQjikl49yV7"
      },
      "source": [
        "# build the model\n",
        "model = unet()\n",
        "sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=dice_coef_loss, optimizer=sgd, metrics=[dice_coef])\n",
        "\n",
        "# fix random seed for reproductibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# FIT THE MODEL - OPTIMIZATION\n",
        "hist = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, batch_size=8, verbose=1)\n",
        "model.save('unet.h5')\n",
        "\n",
        "# 학습과정 살펴보기\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(hist.history['dice_coef'], 'b', label='train dice_coef')\n",
        "acc_ax.plot(hist.history['val_dice_coef'], 'g', label='val dice_coef')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('dice_coef')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsnJnX-_97qb"
      },
      "source": [
        "plotPredictions(X_train, y_train, X_valid, y_valid, X_test, y_test, model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
