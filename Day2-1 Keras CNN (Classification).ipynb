{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/younggon2/Education-ComputerVision-DeepLearning/blob/master/Day2-1%20Keras%20CNN%20(Classification).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8EMukl-uJ0u"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xww_HqaQpyeN"
   },
   "source": [
    "# Keras 실습 (Cont.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNGzA6IYqQ5j"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqAqXTe7qWYk"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()\n",
    "\n",
    "# initialize the label name\n",
    "labelNames = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\",\n",
    "              \"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULBgyQjEqcM8"
   },
   "outputs": [],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "width = height = 28\n",
    "num_pixels = width * height\n",
    "trainX = trainX.reshape(60000, num_pixels).astype('float32') / 255.0\n",
    "testX = testX.reshape(10000, num_pixels).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "valX = trainX[50000:]\n",
    "valY = trainY[50000:]\n",
    "trainX = trainX[:50000]\n",
    "trainY = trainY[:50000]\n",
    "\n",
    "# one hot encode outputs\n",
    "num_classes = 10\n",
    "trainY = tf.keras.utils.to_categorical(trainY, num_classes)\n",
    "valY = tf.keras.utils.to_categorical(valY, num_classes)\n",
    "testY = tf.keras.utils.to_categorical(testY, num_classes)\n",
    "\n",
    "print ('train shape: \\t', trainX.shape)\n",
    "print ('valid shape: \\t', valX.shape)\n",
    "print ('test shape: \\t', testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBfQH1O8fEht"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NBd1mo-p4vt"
   },
   "source": [
    "## STEP 11: 네 번째 인공지능 모델 (Convolutional Neural Network, CNN)\n",
    "![대체 텍스트](https://www.mdpi.com/entropy/entropy-19-00242/article_deploy/html/images/entropy-19-00242-g001.png)\n",
    "**중요! 입력 데이터의 형태가 바뀌어야 한다!!!!**  \n",
    "**784 (1D) -> 28x28 (2D)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPD1Dqw4p4IF"
   },
   "outputs": [],
   "source": [
    "# reshape to be [samples][pixels][width][height]\n",
    "trainX = trainX.reshape(50000, 28, 28, 1)\n",
    "valX = valX.reshape(10000, 28, 28, 1)\n",
    "testX = testX.reshape(10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMA1cyJ3qPW8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "def simple_cnn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (5,5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uahx6ccpqvC3"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = simple_cnn_model()\n",
    "model.summary()\n",
    "\n",
    "# fix random seed for reproductibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=1)\n",
    "model.save('simple_cnn_model.h5')\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 1.5])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.5, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqlqtE2vq-_l"
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(testX, testY, verbose=0)\n",
    "print(\"2D simple CNN error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMYUt5KPrCYE"
   },
   "source": [
    "## STEP 12: Convolution kernel 살펴보기 (5x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4GSDCCHrBj2"
   },
   "outputs": [],
   "source": [
    "W1 = model.layers[0].get_weights()[0]\n",
    "W1 = np.squeeze(W1)\n",
    "\n",
    "print(W1.shape)\n",
    "W1 = np.transpose(W1, (2,0,1))\n",
    "\n",
    "plt.figure(figsize=(5, 5), frameon=False)\n",
    "for ind, val in enumerate(W1):\n",
    "    plt.subplot(6, 6, ind + 1)\n",
    "    im = val.reshape((5,5))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(im, cmap='gray',interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7_utmfwrLxe"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "convout1_f = tf.keras.models.Model([model.layers[0].input], [model.layers[1].output])\n",
    "\n",
    "x_rep = convout1_f([testX[0:3]])\n",
    "x_rep = np.squeeze(x_rep)\n",
    "\n",
    "print(x_rep.shape)\n",
    "\n",
    "for this_x_rep in x_rep:\n",
    "    plt.figure(figsize=(5, 5), frameon=False)\n",
    "\n",
    "    for i in range (this_x_rep.shape[2]):\n",
    "        val = this_x_rep[:,:,i]\n",
    "        plt.subplot(6, 6, i + 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(val, cmap='gray',interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypTufZDDrUF6"
   },
   "source": [
    "## STEP 13: 마지막 인공지능 모델 (VGG-like CNN)\n",
    "![대체 텍스트](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_8r5QRfrZau"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "def cnn_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3,3), input_shape=(28, 28, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "\n",
    "    model.add(Conv2D(32, (3,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "\n",
    "    model.add(Conv2D(64, (3,3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(activation='relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile model\n",
    "    sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['acc'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpzNNU_1ricu"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = cnn_model()\n",
    "\n",
    "# fix random seed for reproductibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Fit the model\n",
    "hist = model.fit(trainX, trainY, validation_data=(valX, valY), epochs=20, batch_size=64, verbose=1)\n",
    "model.save('cnn_model.h5')\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 1.5])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.5, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aiRe7LReroEf"
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(testX, testY, verbose=0)\n",
    "print(\"VGG-like CNN error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNg-_Ll7rp7x"
   },
   "source": [
    "## STEP 14: 결과 확인하기 (틀린 것 들만)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCemHqH_rsHN"
   },
   "outputs": [],
   "source": [
    "# 7. 모델 사용하기\n",
    "yhat_test = model.predict(testX, batch_size=32)\n",
    "\n",
    "plt_row = 5\n",
    "plt_col = 5\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "f, axarr = plt.subplots(plt_row, plt_col)\n",
    "\n",
    "cnt = 0\n",
    "i = 0\n",
    "\n",
    "while cnt < (plt_row*plt_col):\n",
    "\n",
    "    if np.argmax(testY[i]) == np.argmax(yhat_test[i]):\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    sub_plt = axarr[(int)(cnt/plt_row), cnt%plt_col]\n",
    "    sub_plt.axis('off')\n",
    "    sub_plt.imshow(testX[i].reshape(width, height), cmap='gray')\n",
    "    sub_plt_title = 'R: ' + labelNames[np.argmax(testY[i])] + '(%.2f)'% (yhat_test[i][np.argmax(testY[i])]) + ' P: ' + labelNames[np.argmax(yhat_test[i])] + '(%.2f)'% (  yhat_test[i][np.argmax(yhat_test[i])])\n",
    "    sub_plt.set_title(sub_plt_title)\n",
    "\n",
    "    i += 1\n",
    "    cnt += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be_YCs4ou8bM"
   },
   "source": [
    "# Medical Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMDOrndovNyW"
   },
   "source": [
    "## 1. 데이터 준비: MedNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ihc9W4WuxXu"
   },
   "outputs": [],
   "source": [
    "# 데이터 다운로드\n",
    "!wget https://raw.githubusercontent.com/mi2rl/datasets/master/mednist.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2N9OdAkDvRlj"
   },
   "outputs": [],
   "source": [
    "# 압축 풀기\n",
    "!tar xzf mednist.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5fCZ1-jvTZL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "olUqmATbva5T"
   },
   "outputs": [],
   "source": [
    "dataDir = 'resized'                                         # 데이터 위치\n",
    "classNames = sorted(os.listdir(dataDir))                    # 각 클래스의 이름들\n",
    "numClass = len(classNames)                                  # Number of classes = number of subdirectories\n",
    "imageFiles = [[os.path.join(dataDir,classNames[i],x)\n",
    "              for x in os.listdir(os.path.join(dataDir,classNames[i]))]\n",
    "              for i in range(numClass)]                     # 각 클래스 별 파일 이름들\n",
    "numEach = [len(imageFiles[i]) for i in range(numClass)]     # 각 클래스 별 파일 갯수\n",
    "imageFilesList = []                                         # 모든 파일이름\n",
    "imageClass = []                                             # 각각의 파일들에 대한 클래스\n",
    "\n",
    "for i in range(numClass):\n",
    "    imageFilesList.extend(imageFiles[i])\n",
    "    imageClass.extend([i]*numEach[i])\n",
    "\n",
    "numTotal = len(imageClass)                                            # 전체 파일 갯수\n",
    "imageWidth, imageHeight = Image.open(imageFilesList[0]).size          # 각 영상의 사이즈(width, height)\n",
    "\n",
    "print(\"There are\",numTotal,\"images in\",numClass,\"distinct categories\")\n",
    "print(\"Label names:\",classNames)\n",
    "print(\"Label counts:\",numEach)\n",
    "print(\"Image dimensions:\",imageWidth,\"x\",imageHeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgNtmnRdvpGk"
   },
   "outputs": [],
   "source": [
    "# 전체 이미지 중 9개를 랜덤으로 골라 3x3으로 레이블과 함께 그리기\n",
    "# -- 여러번 실행하며 이미지들을 살펴보세요 --\n",
    "\n",
    "plt.subplots(3,3,figsize=(8,8))\n",
    "for i,k in enumerate(np.random.randint(numTotal, size=9)):\n",
    "    im = Image.open(imageFilesList[k])\n",
    "    arr = np.array(im)\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.xlabel(classNames[imageClass[k]])\n",
    "    plt.imshow(arr,cmap='gray',vmin=0,vmax=255)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTQtgO8FvuI7"
   },
   "outputs": [],
   "source": [
    "# 이미지 리스트 살펴보기\n",
    "imageFilesList[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlEO1YKuv0D6"
   },
   "source": [
    "## 2. VGG16을 이용한 분류 실습 (w/ ImageNet pre-trained weight)\n",
    "![대체 텍스트](https://www.cs.toronto.edu/~frossard/post/vgg16/vgg16.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kd4_i0Zmvwbb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9Sen1N2wQvD"
   },
   "source": [
    "### 2.1. [Quiz] 순서가 섞인 layer들을 VGG16 구성에 맞게 배치해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mgV1yNkVwP07"
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(224, 224, 3,), name=\"VGGInput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFQT3zY6wdmZ"
   },
   "outputs": [],
   "source": [
    "x = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = MaxPool2D(padding='same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4S7dx4ukwW8b"
   },
   "outputs": [],
   "source": [
    "x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = MaxPool2D(padding='same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xse04pRJwV4D"
   },
   "outputs": [],
   "source": [
    "x = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = MaxPool2D(padding='same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pB1T6s7_wbEz"
   },
   "outputs": [],
   "source": [
    "x = Conv2D(filters=4096, kernel_size=(7,7), padding='valid', activation='relu')(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(4096, activation='relu')(x)\n",
    "pred = Dense(1000, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_HAox3Zwe2j"
   },
   "outputs": [],
   "source": [
    "x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = MaxPool2D(padding='same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZKko20twf2Q"
   },
   "outputs": [],
   "source": [
    "x = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(inputs)\n",
    "x = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(x)\n",
    "x = MaxPool2D(padding='same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2C7UeJiAwhl5"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "im31ZQYSwior"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8k-vSvewkvB"
   },
   "source": [
    "### 2.2. VGG16 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wuCWw4dwmh1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import vgg16\n",
    "\n",
    "# VGG16 모델 불러오기\n",
    "model = vgg16.VGG16()\n",
    "\n",
    "# 모델의 모양을 보여준다.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGxz1TcSwrGD"
   },
   "outputs": [],
   "source": [
    "# Model 구성도 plot\n",
    "from IPython.display import Image\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file='vgg16.png', show_shapes=True, show_layer_names=True)\n",
    "Image(filename='vgg16.png', width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1u0FE0Owzza"
   },
   "source": [
    "**VGG16**\n",
    "\n",
    "`keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)`\n",
    "\n",
    "VGG16 model, with weights pre-trained on ImageNet.\n",
    "\n",
    "This model can be built both with 'channels_first' data format (channels, height, width) or 'channels_last' data format (height, width, channels).\n",
    "\n",
    "The default input size for this model is 224x224.\n",
    "\n",
    "**Arguments**\n",
    "\n",
    "* include_top: whether to include the 3 fully-connected layers at the top of the network.\n",
    "* weights: one of None (random initialization) or 'imagenet' (pre-training on ImageNet).\n",
    "* input_tensor: optional Keras tensor (i.e. output of layers.Input()) to use as image input for the model.\n",
    "* input_shape: optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with 'channels_last' data format) or (3, 224, 224) (with 'channels_first' data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. (200, 200, 3) would be one valid value.\n",
    "* pooling: Optional pooling mode for feature extraction when include_top is False.\n",
    "* classes: optional number of classes to classify images into, only to be specified if include_top is True, and if no weights argument is specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-qQ5MKjw-QJ"
   },
   "source": [
    "**tf.keras에서 제공되는 모델들 참고:** https://www.tensorflow.org/api_docs/python/tf/keras/applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryKA-4jCwwZj"
   },
   "outputs": [],
   "source": [
    "# VGG16 모델을 이용해 prediction 하는 함수\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from IPython.display import display # 이미지 출력 함수\n",
    "\n",
    "def predict_vgg16(model, filename) :\n",
    "    # 이미지 파일을 읽고 화면에 표시\n",
    "    image = load_img(filename)\n",
    "    display(image)\n",
    "\n",
    "    # 모델 사이즈로 이미지 파일을 읽기\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "\n",
    "    # 이미지 데이터를 numpy로 변환\n",
    "    image = img_to_array(image)\n",
    "\n",
    "    # vgg16.preprocess_input()을 호출하기 위해 차원을 조정\n",
    "    # 보통 모델을 여러 이미지를 한번에 호출.\n",
    "    # 맨 앞의 1 : 이미지 갯수가 1개라는 것.\n",
    "    # 두번째 224 : 가로\n",
    "    # 세번째 224 : 세로\n",
    "    # 네번째 3 : R, G, B 3개\n",
    "    image = image.reshape((1, 224, 224, 3))\n",
    "\n",
    "    # VGG16 모델 호출을 위해 데이터 전처리.\n",
    "    # -255 ~ 255 사이 값으로 정규화한다.\n",
    "    # 그리고 RGB를 BGR순으로 바꾼다.\n",
    "    image = vgg16.preprocess_input(image)\n",
    "\n",
    "\n",
    "    # 이미지를 모델에 적용\n",
    "    yhat = model.predict(image)\n",
    "\n",
    "    # 모델 적용된 결과를 파싱\n",
    "    label = vgg16.decode_predictions(yhat)\n",
    "\n",
    "    # 가장 확률이 높은 결과를 획득\n",
    "    label = label[0][0]\n",
    "\n",
    "    # 라벨과 라벨을 예측한 확률을 출력\n",
    "    print('%s (%.2f%%)' % (label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmSvgK-AxPGj"
   },
   "outputs": [],
   "source": [
    "files = imageFilesList[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CDLi-6BxQu0"
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "  predict_vgg16(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gASz7eGXxoAe"
   },
   "source": [
    "### 2.3. Dataset 나누기: Train / Validation / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fst_gknExSB7"
   },
   "outputs": [],
   "source": [
    "validFrac = 0.2   # Define the fraction of images to move to validation dataset\n",
    "testFrac = 0.2    # Define the fraction of images to move to test dataset\n",
    "validList = []\n",
    "testList = []\n",
    "trainList = []\n",
    "\n",
    "for i in range(numTotal):\n",
    "    rann = np.random.random() # Randomly reassign images\n",
    "    if rann < validFrac:\n",
    "        validList.append(i)\n",
    "    elif rann < testFrac + validFrac:\n",
    "        testList.append(i)\n",
    "    else:\n",
    "        trainList.append(i)\n",
    "\n",
    "nTrain = len(trainList)  # Count the number in each set\n",
    "nValid = len(validList)\n",
    "nTest = len(testList)\n",
    "print(\"Training images =\",nTrain,\"\\nValidation =\",nValid,\"\\nTesting =\",nTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enmIECjhxskj"
   },
   "outputs": [],
   "source": [
    "!mkdir ./train\n",
    "!mkdir ./valid\n",
    "!mkdir ./test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2doy2mnoxxDL"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(len(trainList))):\n",
    "  root, clas, src = imageFilesList[trainList[i]].split('/')\n",
    "  dest = os.path.join('./train',clas,src)\n",
    "  if not os.path.exists(os.path.join('./train',clas)):\n",
    "    os.mkdir(os.path.join('./train',clas))\n",
    "  shutil.copy(imageFilesList[trainList[i]], dest)\n",
    "\n",
    "for i in tqdm(range(len(validList))):\n",
    "  root, clas, src = imageFilesList[validList[i]].split('/')\n",
    "  dest = os.path.join('./valid',clas,src)\n",
    "  if not os.path.exists(os.path.join('./valid',clas)):\n",
    "    os.mkdir(os.path.join('./valid',clas))\n",
    "  shutil.copy(imageFilesList[validList[i]], dest)\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(testList))):\n",
    "  root, clas, src = imageFilesList[testList[i]].split('/')\n",
    "  dest = os.path.join('./test',clas,src)\n",
    "  if not os.path.exists(os.path.join('./test',clas)):\n",
    "    os.mkdir(os.path.join('./test',clas))\n",
    "  shutil.copy(imageFilesList[testList[i]], dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIRAsA6mx6l5"
   },
   "source": [
    "### 2.4. Image Data Generator 정의 (+Data Augmentation)\n",
    "**Keras API - ImageDataGenerator: 일정한 규칙으로 만들어진 폴더구조에서 데이터셋을 자동으로 불러와 학습에 사용할 수 있게 도와주는 API**\n",
    "![대체 텍스트](https://miro.medium.com/max/875/1*HpvpA9pBJXKxaPCl5tKnLg.jpeg)\n",
    "https://medium.com/@vijayabhaskar96/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720  \n",
    "**Data augmentation: 데이터에 다양한 형태의 변화를 임의로 생성하여 데이터의 갯수와 다양성을 증가시키는 방법**\n",
    "![대체 텍스트](https://miro.medium.com/max/1250/1*rvwzKkvhlDN3Wo_4Oay_4Q.png)\n",
    "https://medium.com/@thimblot/data-augmentation-boost-your-image-dataset-with-few-lines-of-python-155c2dc1baec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1K1bqmTx4JW"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "def preprocess_input_vgg(x):\n",
    "    X = np.expand_dims(x, axis=0)\n",
    "    X = preprocess_input(X)\n",
    "    return X[0]\n",
    "\n",
    "\n",
    "train_dir = './train'\n",
    "validation_dir = './valid'\n",
    "test_dir = './test'\n",
    "batch_size = 32\n",
    "image_size = 224\n",
    "\n",
    "# 학습에 사용될 이미지 데이터 생성기\n",
    "train_datagen = ImageDataGenerator(\n",
    "      preprocessing_function=preprocess_input_vgg,\n",
    "      rotation_range=20, # 회전 최대 20도\n",
    "      width_shift_range=0.2, # 좌우 이동\n",
    "      height_shift_range=0.2, # 상하 이동\n",
    "      horizontal_flip=True, # 좌우 반전\n",
    "      vertical_flip=True, # 상하 반전\n",
    "      )\n",
    "\n",
    "# 검증에 사용될 이미지 데이터 생성기\n",
    "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg)\n",
    "\n",
    "# 테스트에 사용될 이미지 데이터 생성기\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input_vgg)\n",
    "\n",
    "\n",
    "# 학습에 사용될 데이터 생성기\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "\n",
    "# 검증에 사용될 데이터 생성기\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_num=len(train_generator.class_indices)\n",
    "\n",
    "custom_labels = list(validation_generator.class_indices.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5wWZ2jjyhPt"
   },
   "source": [
    "### 2.5. VGG16 as a Feature Extractor\n",
    "![대체 텍스트](https://miro.medium.com/max/875/1*W91k18rRAZfJnsM8bhUDXA.png)\n",
    "https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjZrLwH_ybj6"
   },
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "vgg_model = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "vgg_model.summary()\n",
    "\n",
    "# Convolution Layer를 학습되지 않도록 고정\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-ne4ntHywoq"
   },
   "outputs": [],
   "source": [
    "# 새로운 모델 생성하기\n",
    "last = vgg_model.output\n",
    "\n",
    "# VGG16모델에 Fully Connected부분을 재구성해서 추가\n",
    "x = Flatten()(last)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "pred = Dense(class_num, activation='softmax')(x)\n",
    "\n",
    "model = Model(vgg_model.input, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duMoZmLUyztK"
   },
   "outputs": [],
   "source": [
    "# 새로운 모델 요약\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OLUpE4Yy0-y"
   },
   "outputs": [],
   "source": [
    "# 새로운 모델 저장\n",
    "vgg16_model_path = 'vgg16_finetuning.h5'\n",
    "\n",
    "model.save(vgg16_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoLo279Fy3BF"
   },
   "source": [
    "### 2.6. 새로운 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GnTmLhry2EN"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# 모델 로딩\n",
    "model = load_model(vgg16_model_path)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "      x=train_generator,\n",
    "      steps_per_epoch=100 ,\n",
    "      epochs=2,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=math.ceil(validation_generator.samples/validation_generator.batch_size),\n",
    "      verbose=1)\n",
    "\n",
    "# 모델 저장\n",
    "model.save(vgg16_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-M3ddWJzIVj"
   },
   "source": [
    "### 2.7. 학습 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WB_zw59fy8YZ"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    "valacc = history.history['val_acc']\n",
    "valloss = history.history['val_loss']\n",
    "\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='accuracy')\n",
    "plt.plot(epochs, loss, 'r', label='loss')\n",
    "plt.plot(epochs, valacc, 'b--', label='val_accuracy')\n",
    "plt.plot(epochs, valloss, 'r--', label='val_loss')\n",
    "plt.title('accuracy and loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQlJsL1WzL43"
   },
   "source": [
    "### 2.8. 학습된 모델을 이용해 Test 데이터에 대한 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfcBgWbmxkYi"
   },
   "outputs": [],
   "source": [
    "# 테스트에 사용될 데이터 생성기\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=64,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqhbuTwvzLXK"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_generator, steps=math.ceil(test_generator.samples/test_generator.batch_size), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eY3Otk4Kzr_X"
   },
   "source": [
    "## 3. VGG16을 이용한 분류 실습 (from Scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4yPCgVpzQeq"
   },
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "vgg_model = vgg16.VGG16(weights=None, include_top=False, input_shape=(image_size, image_size, 3))\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJ6bc8mszxZr"
   },
   "outputs": [],
   "source": [
    "last = vgg_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djUQpx7izy6e"
   },
   "outputs": [],
   "source": [
    "# VGG16모델에 Fully Connected부분을 재구성해서 추가\n",
    "x = Flatten()(last)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "pred = Dense(class_num, activation='softmax')(x)\n",
    "\n",
    "model = Model(vgg_model.input, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BSzO27uz2tC"
   },
   "outputs": [],
   "source": [
    "# 새로운 모델 요약\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_5nCl3Kz3q0"
   },
   "outputs": [],
   "source": [
    "# 새로운 모델 저장\n",
    "vgg16_model_path = 'vgg16_scratch.h5'\n",
    "\n",
    "model.save(vgg16_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFybSYyez5lA"
   },
   "outputs": [],
   "source": [
    "# 모델 로딩\n",
    "model = load_model(vgg16_model_path)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(learning_rate=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100 ,\n",
    "      epochs=2,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=math.ceil(validation_generator.samples/validation_generator.batch_size),\n",
    "      verbose=1)\n",
    "\n",
    "# 모델 저장\n",
    "model.save(vgg16_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-8uXpdAz9TP"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    "valacc = history.history['val_acc']\n",
    "valloss = history.history['val_loss']\n",
    "\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='accuracy')\n",
    "plt.plot(epochs, loss, 'r', label='loss')\n",
    "plt.plot(epochs, valacc, 'b--', label='val_accuracy')\n",
    "plt.plot(epochs, valloss, 'r--', label='val_loss')\n",
    "plt.title('accuracy and loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5GjsfwR9z_D-"
   },
   "outputs": [],
   "source": [
    "# For prediction purposes\n",
    "y_pred = model.predict(test_generator, steps=math.ceil(test_generator.samples/test_generator.batch_size), verbose=1)\n",
    "y_pred1 = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLVgvhhL0wct"
   },
   "source": [
    "## 4. Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipuSDuzr0y1Z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzImpuN801pP"
   },
   "outputs": [],
   "source": [
    "# y_test labeling\n",
    "y_test = test_generator.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNLX571s029h"
   },
   "outputs": [],
   "source": [
    "# calculate confusion matrix for the predicted dataset\n",
    "cm = confusion_matrix(y_test, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11XFlnUZ06Z7"
   },
   "outputs": [],
   "source": [
    "# make a dataframe using cm array\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in classNames], columns = [i for i in classNames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2FhkKjI07Ig"
   },
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "plt.figure(figsize = (10, 7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3T8T24eE07np"
   },
   "outputs": [],
   "source": [
    "# classification report generation: precision, recall, f1-score.\n",
    "print(classification_report(y_test, y_pred1, target_names=classNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTeOtGDW1Bcq"
   },
   "source": [
    "## 5. VGG16 + Grad-CAM\n",
    "![대체 텍스트](https://camo.githubusercontent.com/450498bd998fd99d51b647d2b6c8631e94585522/687474703a2f2f692e696d6775722e636f6d2f4a614762645a352e706e67)\n",
    "**Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization**  \n",
    "Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, Dhruv Batra  \n",
    "https://arxiv.org/abs/1610.02391  \n",
    "  \n",
    "**Example: 'Boxer'**  \n",
    "![대체 텍스트](https://github.com/PowerOfCreation/keras-grad-cam/raw/master/examples/cat_dog.png)\n",
    "![대체 텍스트](https://github.com/PowerOfCreation/keras-grad-cam/raw/master/examples/cat_dog_242_gradcam.jpg)\n",
    "![대체 텍스트](https://github.com/PowerOfCreation/keras-grad-cam/raw/master/examples/cat_dog_242_guided_gradcam.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gr02n5Y95Xyq"
   },
   "outputs": [],
   "source": [
    "inum = random.randrange(0,len(imageFilesList))\n",
    "qimage0 = load_img(imageFilesList[inum], target_size=(224, 224))\n",
    "qimage  = img_to_array(qimage0)\n",
    "qimage  = qimage.reshape((1, 224, 224, 3))\n",
    "qimage  = vgg16.preprocess_input(qimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfkGzgoU5YqL"
   },
   "outputs": [],
   "source": [
    "plt.imshow(qimage0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq6GXKb51ss1"
   },
   "outputs": [],
   "source": [
    "def grad_cam(input_model, image):\n",
    "    nb_classes = 6  # 클래스 숫자\n",
    "    preds = input_model.predict(image)\n",
    "    predicted_label = np.argmax(preds[0])\n",
    "\n",
    "    grad_model = tf.keras.Model(\n",
    "        [input_model.input], [input_model.get_layer('block5_conv3').output, input_model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        conv_outputs, predictions = grad_model(image)\n",
    "\n",
    "        if isinstance(predictions, list):\n",
    "            predictions = predictions[0]\n",
    "\n",
    "        loss = predictions[:, predicted_label]\n",
    "\n",
    "    grad = tape.gradient(loss, conv_outputs)[0]\n",
    "    weights = tf.reduce_sum(grad, axis=(0, 1))\n",
    "    cam = tf.reduce_sum(tf.multiply(weights, conv_outputs[0]), axis=-1)\n",
    "    cam = cv2.resize(cam.numpy(), (224, 224))\n",
    "    cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "    cam = cv2.applyColorMap(\n",
    "        cv2.cvtColor((cam * 255).astype(\"uint8\"), cv2.COLOR_GRAY2BGR), cv2.COLORMAP_JET\n",
    "    )\n",
    "\n",
    "    return np.uint8(cam), predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7q5lwkc3O5g"
   },
   "outputs": [],
   "source": [
    "inum = random.randrange(0,len(imageFilesList))\n",
    "qimage0 = load_img(imageFilesList[inum], target_size=(224, 224))\n",
    "qimage  = img_to_array(qimage0)\n",
    "qimage  = qimage.reshape((1, 224, 224, 3))\n",
    "qimage  = vgg16.preprocess_input(qimage)\n",
    "\n",
    "# CAM 출력\n",
    "cam, plabel = grad_cam(model, qimage)\n",
    "plt.title('T:'+classNames[imageClass[inum]]+', P:'+classNames[plabel])\n",
    "plt.imshow(qimage0)\n",
    "plt.imshow(cam, alpha=.5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "Day2-1 CNN (Classification).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyTorch 2.2 (NGC 23.11/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
